{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a8d7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poswalabhishek/opt/anaconda3/lib/python3.9/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from database import database\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import pyLDAvis.gensim_models\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc29ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Ownership change\", \"Change of control\", \"Acceleration\", \"accelerate\", \"Default\", \\\n",
    "            \"Insolvency\", \"Insolvent\", \"Delay\", \"Late\", \"Failure\", \"fail\", \"Dispute\", \"Liquidation\", \\\n",
    "            \"Liquidator\", \"Margin call\", \"Haircut\", \"Bank run\", \"Termination\", \"Moratorium\", \"Suspension\", \\\n",
    "            \"Suspend\", \"Fraud\", \"misrepresentation\", \"Fine\", \"sanction\", \"Breach\", \"Reschedule\", \"Restructuring\", \\\n",
    "            \"Restructure\", \"Credit event\", \"Losses\", \"Loss\", \"Bailout\", \"Bailin\", \"Bankrupt\", \"Receivership\", \\\n",
    "            \"Receiver\", \"Judicial Management\", \"Judicial Manager\", \"Administration\", \"Administrator\", \"Sequestrate\", \\\n",
    "            \"Sequestration\", \"Support\", \"Capital call\", \"Liquidity event\", \"Negative trends\", \"Price changes\", \\\n",
    "            \"Board infighting\", \"Corruption\", \"Inappropriate or ultra vires dealings\", \"Negative working capital\", \\\n",
    "            \"Acquisition\", \"LBO\", \"Qualified audit opinion\", \"Regulatory breach\", \"Non-performing assets\", \\\n",
    "            \"Provisions\", \"Force majeur\", \"Distress\", \"Frozen\", \"Delisted\", \"Sued\", \"Suit\", \"Arrested\", \\\n",
    "            \"Disappeared\", \"Uncontactable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a66bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAModel:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9326f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize (sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(sentence) # Tokenize: Split the sentence into words\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True)) # deacc removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "263bf133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe (news_df, counterparty: str):\n",
    "    \n",
    "    '''\n",
    "    lose the irrelevant columns\n",
    "    remove punctuations\n",
    "    lowercase\n",
    "    combine summary and headline for news\n",
    "    convert counterpary articles to words\n",
    "    remove stopwords\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    news_df = news_df.drop(columns=['_id', 'url', 'image', 'source', 'api'], axis=1).sample(100)\n",
    "    news_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    news_df[\"headline\"] = news_df[\"headline\"].map(lambda x: re.sub(r'[,\\.!?]', '', x))\n",
    "    news_df[\"headline\"] = news_df[\"headline\"].map(lambda x: x.lower())\n",
    "    news_df[\"summary\"] = news_df[\"summary\"].map(lambda x: re.sub(r'[,\\.!?]', '', x))\n",
    "    news_df[\"summary\"] = news_df[\"summary\"].map(lambda x: x.lower())\n",
    "    \n",
    "    counterparty_news = news_df[news_df[\"counterparty\"] == counterparty]\n",
    "    counterparty_news = counterparty_news[\"headline\"] + counterparty_news[\"summary\"]\n",
    "    counterparty_news = counterparty_news.values.tolist()\n",
    "    \n",
    "    counterparty_article_to_words = list(sent_to_words(counterparty_news))\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    tokenized_counterparty_news = [[word for word in simple_preprocess(str(news)) if word not in stop_words \\\n",
    "                                    and len(word) > 4] for news in counterparty_article_to_words]\n",
    "    \n",
    "    return tokenized_counterparty_news\n",
    "    \n",
    "    \n",
    "def get_news_dataframe (counterparty_name: str):\n",
    "    news_doc = []\n",
    "    for doc in database.news_collection.find({}):\n",
    "        news_doc.append(doc)\n",
    "    news_df = pd.DataFrame(news_doc)\n",
    "    \n",
    "    news_df = preprocess_dataframe(news_df, counterparty_name)\n",
    "    \n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a4199f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tesla_news = get_news_dataframe('TSLA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdaa433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model (counterparty: str, counterparty_news, num_topics):\n",
    "    id2word = corpora.Dictionary(counterparty_news) # Create Dictionary\n",
    "    articles = counterparty_news # Create Corpus\n",
    "    corpus = [id2word.doc2bow(article) for article in articles] # Term Document Frequency\n",
    "    \n",
    "\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics) # Build LDA model\n",
    "    \n",
    "    \n",
    "\n",
    "    pprint(lda_model.print_topics()) # Print the Keyword in the num topics\n",
    "    \n",
    "    path = './lda_results/ldavis_' # have this path on your computer -> will point to the database in future\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "    LDAvis_data_filepath = os.path.join(path + str(num_topics) + counterparty)\n",
    "    if 1 == 1:\n",
    "        LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds='mmds')\n",
    "        with open(LDAvis_data_filepath, 'wb') as f:\n",
    "            pickle.dump(LDAvis_prepared, f)\n",
    "    \n",
    "#     LDAvis_prepared\n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5200bdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model ('TSLA', tesla_news, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00b4fb9",
   "metadata": {},
   "source": [
    "## Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d29ea05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d36d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize (articles):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    tokens = [word for word in nltk.word_tokenize(articles) if (len(word) > 4) ] \n",
    "    stems = [stemmer.lemmatize(item) for item in tokens]\n",
    "    return stems\n",
    "\n",
    "def preprocess_dataframe (news_df, counterparty_name: str):\n",
    "    \n",
    "    '''\n",
    "    lose the irrelevant columns\n",
    "    remove punctuations\n",
    "    lowercase\n",
    "    combine summary and headline for news\n",
    "    convert counterpary articles to words\n",
    "    remove stopwords\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    news_df = news_df.drop(columns=['_id', 'url', 'image', 'source', 'api'], axis=1)\n",
    "    news_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    news_df[\"headline\"] = news_df[\"headline\"].map(lambda x: re.sub(r'[,\\.!?]', '', x))\n",
    "    news_df[\"headline\"] = news_df[\"headline\"].map(lambda x: x.lower())\n",
    "    news_df[\"summary\"] = news_df[\"summary\"].map(lambda x: re.sub(r'[,\\.!?]', '', x))\n",
    "    news_df[\"summary\"] = news_df[\"summary\"].map(lambda x: x.lower())\n",
    "\n",
    "    news_df = news_df[news_df[\"counterparty\"] == counterparty_name]\n",
    "    \n",
    "    X_train, X_test = train_test_split(news_df, test_size=0.7, random_state=4201)\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def get_news_dataframe (counterparty_name: str):\n",
    "    news_doc = []\n",
    "    for doc in database.news_collection.find({}):\n",
    "        news_doc.append(doc)\n",
    "    news_df = pd.DataFrame(news_doc)\n",
    "    \n",
    "    train_news_df, test_news_df = preprocess_dataframe(news_df, counterparty_name)\n",
    "    return train_news_df, test_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eceada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modelled (lda, train_news_df, test_news_df, vectorizer_tf, W1, H1):\n",
    "    \n",
    "    colnames = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(train_news_df.summary))]\n",
    "    df_doc_topic_train = pd.DataFrame(np.round(W1, 2), columns=colnames, index=docnames)\n",
    "    significant_topic = np.argmax(df_doc_topic_train.values, axis=1)\n",
    "    df_doc_topic_train['dominant_topic'] = significant_topic\n",
    "    \n",
    "    WHold = lda.transform(vectorizer_tf.transform(test_news_df.headline + test_news_df.summary))\n",
    "    \n",
    "    colnames = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
    "    docnames = [\"Doc\" + str(i) for i in range(len(test_news_df.summary))]\n",
    "    df_doc_topic_test = pd.DataFrame(np.round(WHold, 2), columns=colnames, index=docnames)\n",
    "    significant_topic = np.argmax(df_doc_topic_test.values, axis=1)\n",
    "    df_doc_topic_test['dominant_topic'] = significant_topic\n",
    "\n",
    "    return df_doc_topic_train, df_doc_topic_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d6bf378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_lda_model (counterparty_name: str, num_topic_words = 10, num_topics = 10):\n",
    "    train_news_df, test_news_df = get_news_dataframe (counterparty_name)\n",
    "    \n",
    "    \n",
    "    vectorizer_tf = TfidfVectorizer(tokenizer = tokenize, stop_words = 'english',\\\n",
    "                                    max_features = 1000, use_idf = False, norm = None)\n",
    "    tf_vectors = vectorizer_tf.fit_transform(train_news_df.headline + train_news_df.summary) \n",
    "    \n",
    "    lda = decomposition.LatentDirichletAllocation(n_components = num_topics,\\\n",
    "                                              max_iter = 3, learning_method = 'online',\\\n",
    "                                              learning_offset = 50, n_jobs = -1, random_state=4201)\n",
    "    \n",
    "    W1 = lda.fit_transform(tf_vectors)\n",
    "    H1 = lda.components_\n",
    "    vocab = np.array(vectorizer_tf.get_feature_names())\n",
    "\n",
    "    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_topic_words-1:-1]]\n",
    "    topic_words = ([top_words(t) for t in H1])\n",
    "    topics = [' '.join(t) for t in topic_words]\n",
    "    \n",
    "    df_doc_topic_train, df_doc_topic_test = topic_modelled (lda, train_news_df, test_news_df, vectorizer_tf, W1, H1)\n",
    "    \n",
    "    return topics, df_doc_topic_train, df_doc_topic_test, train_news_df, test_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca1ceb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_topic_per_article (counterparty_name: str, num_topic_words = 10, num_topics = 10):\n",
    "    topics, df_doc_topic_train, df_doc_topic_test, train_news_df, test_news_df = advanced_lda_model \\\n",
    "                                                                (counterparty_name, num_topic_words, num_topics)\n",
    "    test_news_df['dominant_topic'] = list(df_doc_topic_test['dominant_topic'])\n",
    "    train_news_df['dominant_topic'] = list(df_doc_topic_train['dominant_topic'])\n",
    "    \n",
    "    test_news_df.reset_index(drop=True, inplace=True)\n",
    "    train_news_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    frames = [test_news_df, train_news_df]\n",
    "    counterparty_news = pd.concat(frames)\n",
    "    counterparty_news.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return topics, counterparty_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced8403",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, counterparty_news = define_topic_per_article('TSLA', 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f41cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterparty_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167693e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph():\n",
    "    topics, counterparty_news = define_topic_per_article('TSLA', 8, 8)\n",
    "    num_topics = len(topics)\n",
    "    \n",
    "    topics_dict_count = {}\n",
    "    \n",
    "    for topic_num in range(num_topics):\n",
    "        topic_dict_count[topic_num] = \\\n",
    "        counterparty_news[counterparty_news[\"dominant_topic\"] == topic_num].dominant_topic.count()\n",
    "    \n",
    "    topics_list = topics_dict_count.items()\n",
    "    x, y = zip(*topics_list) \n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,3)\n",
    "    plt.plot(x, y)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf9645",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
